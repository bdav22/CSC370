{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdav22/CSC370/blob/master/HW_X_Spark_RDD_Word_Count_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW_X: Word Count\n",
        "    - 10 points\n",
        "    - due by the midnight on April 26.\n",
        "    - submit your notebook to the Blackboard.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Requirments: You need to use PySpark RDD to do the HW. \n",
        "\n",
        "    0. [1 points]  Spark Session Creation\n",
        "        - use local mode\n",
        "        - use Spark Context to create rdd \n",
        "    \n",
        "    1. [1 point] Upload the \"Shakespeare.txt\" to Colab file system\n",
        "        you need to download the book from the BB to your computer first\n",
        "        - check how many lines\n",
        "        - print out the first 10 records\n",
        "    \n",
        "    2. [6 points] Word count\n",
        "        - use MapReduce\n",
        "        - sort by count\n",
        "\n",
        "    3. [1 point] Save a csv file to disk\n",
        "        - convert to Pandas DataFrame\n",
        "        - save to colab\n",
        "\n",
        "    4. [1 point] Submit 2 files to the BB\n",
        "        a. your notebook without any errors\n",
        "        b. the csv file which stors the word count (descending order by count)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "RDD Ref: https://sparkbyexamples.com/pyspark-rdd/\n"
      ],
      "metadata": {
        "id": "N7-cWIeAqwKy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG18JzzenHSU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 0. Spark Session Creation\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSUkv7shnp3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7d51a4-3c0a-4fd5-ce9b-cc523f53df51"
      },
      "source": [
        "!pip install pyspark  # Install PySpark\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=ac0ee961d0931d8c4a355fc7819b6d2b0a38f5c029018b4b9c95fa35ede87100\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession first\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "\n",
        "\n",
        "# Create Spark Session first using local master and word_count as the app name.\n",
        "spark = SparkSession.builder.appName(\"word_count\").getOrCreate()\n",
        "\n",
        "# display spark session \n",
        "spark\n",
        "\n",
        "# word_count.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "WbVNkHukpDDj",
        "outputId": "bd1c8f14-2741-4ed1-dca5-a71f987d9a48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f168d527310>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5d465025328d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>word_count</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKGn03TB54W0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##  1. Create An RDD using Spark Context by reading in file\n",
        "    \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEbnr5mv3wCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529733cf-43c5-47d4-b332-cf010503dfcf"
      },
      "source": [
        "# (1) upload  \"shakespears.txt\" to the Colab default location\n",
        "# on the left panel, click the upload icon \n",
        "\n",
        "\n",
        "# (2) read in local file, generate a RDD, and call it rdd\n",
        "wordFile = \"shakespeare.txt\"\n",
        "rdd =  spark.read.text(wordFile).rdd\n",
        "\n",
        "\n",
        "# display the type of rdd\n",
        "print(type(rdd))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count how many lines: total 4155 lines\n",
        "line_count = rdd.count()\n",
        "print(\"Line count: \", line_count)\n",
        "\n",
        "\n",
        "\n",
        "# take the first 10 records \n",
        "first10 = rdd.take(10)\n",
        "for record in first10:\n",
        "  print(record)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr-4ArU0DaIh",
        "outputId": "630a9f29-33ef-4709-ae00-d0b4e0cf99ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line count:  4155\n",
            "Row(value=\"All's Well That Ends Well\")\n",
            "Row(value=\"Shakespeare homepage | All's Well That Ends Well | Entire play\")\n",
            "Row(value='ACT I')\n",
            "Row(value='')\n",
            "Row(value=\"SCENE I. Rousillon. The COUNT's palace.\")\n",
            "Row(value='')\n",
            "Row(value='Enter BERTRAM, the COUNTESS of Rousillon, HELENA, and LAFEU, all in black')\n",
            "Row(value='COUNTESS')\n",
            "Row(value='In delivering my son from me, I bury a second husband.')\n",
            "Row(value='BERTRAM')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## 2. Word Count using MapReduce\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DvTuwiXKNXMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split each line to single word using split() method, name the new rdd as word_list\n",
        "word_list = rdd.map(lambda line: line.value.split())\n",
        "\n",
        "\n",
        "# print the number of words in word_list: 24251\n",
        "words = word_list.flatMap(lambda word_list: word_list)\n",
        "word_count = words.count()\n",
        "print(\"Word count\", word_count)\n",
        "\n",
        "\n",
        "# display the first 10 words in word_list\n",
        "first10Words = word_list.flatMap(lambda word_list: word_list).take(10)\n",
        "print(\"First 10 words: \", first10Words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ofYM31mThX",
        "outputId": "6ab37cb7-433f-42a8-9459-cc4943b8586e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count 24251\n",
            "First 10 words:  [\"All's\", 'Well', 'That', 'Ends', 'Well', 'Shakespeare', 'homepage', '|', \"All's\", 'Well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map each word to low case using lower() method, name the new RDD as low_list\n",
        "low_list = word_list.flatMap(lambda word_list: word_list).map(lambda word: word.lower())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display the first 5\n",
        "first5Low = low_list.take(5)\n",
        "first5Low\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_-Uz4ry2ZsV",
        "outputId": "b7e1da0b-ecde-41ee-f9b9-db4c1e06f4eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"all's\", 'well', 'that', 'ends', 'well']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all punctions in the low_list using strip(puntuation) method, name it as low_word\n",
        "from string import punctuation\n",
        "import string\n",
        "\n",
        "low_word = low_list.map(lambda word: word.strip(string.punctuation))\n",
        "\n",
        "# count how many words in the RDD low_word: 24251\n",
        "low_count = low_word.count()\n",
        "print(\"Num of words: \", low_count)\n",
        "# the first 10:\n",
        "print(low_word.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUCP-z5LVpp",
        "outputId": "69dbc011-f9a3-43b8-b965-44fe245959e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of words:  24251\n",
            "[\"all's\", 'well', 'that', 'ends', 'well', 'shakespeare', 'homepage', '', \"all's\", 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that there are empty string in the RDD low_word, remove all missing values using filter \n",
        "word_clear = low_word.filter(lambda x: x != '')\n",
        "\n",
        "\n",
        "# count how many words in word_clear: 24246\n",
        "clear_count = word_clear.count()\n",
        "print(\"Num of words: \", clear_count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIxkAW3DAsoO",
        "outputId": "e85e5e8d-2bf8-4252-fa2c-4843e41ed063"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of words:  24246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkE6iQ3f5RfU",
        "outputId": "0612f5f0-7080-4333-8a84-39c7d60bd7de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Word Count: map each word to 1, reduce all the words based on Key,  name it as unique_count\n",
        "unique_count = word_clear.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "\n",
        "\n",
        "# count how many unique word in unique_count: 3562\n",
        "print(\"Unique words: \", unique_count.count())\n",
        "\n",
        "\n",
        "# display the first 5 \n",
        "print(\"First 5: \", unique_count.take(5))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words:  3562\n",
            "First 5:  [(\"all's\", 5), ('well', 84), ('that', 325), ('ends', 5), ('shakespeare', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsVbSbB7NGzL",
        "outputId": "715ae7c3-a87c-404a-89b2-473df33ddd5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sort the RDD unique_count by count, name it sorted_word\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print most frequent 10 words\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(731, 'the'), (708, 'i'), (620, 'and'), (518, 'to'), (484, 'you'), (459, 'of'), (453, 'a'), (378, 'my'), (325, 'that'), (300, 'in')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnN4en53Pmga",
        "outputId": "e229180a-50f9-48f7-9f05-f0a12fabb255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# using map to reverse the RDD sorted_words order (count, word) to (word, count), name it as final_word\n",
        "\n",
        "\n",
        "\n",
        "# display the first 10\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 731), ('i', 708), ('and', 620), ('to', 518), ('you', 484), ('of', 459), ('a', 453), ('my', 378), ('that', 325), ('in', 300)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 3. Save a csv file to disk\n",
        "        - convert to Pandas DataFrame\n",
        "        - save the dataframe to the colab using the name \"word_count.csv\"\n",
        "        - download the csv file to your computer\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hU8D7K8sJuVM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcYavjgJemb"
      },
      "source": [
        "# RDD to Spark DF\n",
        "\n",
        "\n",
        "# to Pandas DF and save it to colab\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### 4. Submit 2 files to the BB\n",
        "\n",
        "    a. this notebook (rerun your code to make sure there is no error) and\n",
        "    b. \"word_count.csv\" file \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_PtUakVmJ-LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop your spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "fj-mlxjiKotz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}